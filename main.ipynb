{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e809c4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hdf5storage\n",
    "\n",
    "from Neural_Decoding.preprocessing_funcs import bin_spikes\n",
    "from Neural_Decoding.preprocessing_funcs import bin_output\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import tensorflow.keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "#Import function to get the covariate matrix that includes spike history from previous bins\n",
    "from Neural_Decoding.preprocessing_funcs import get_spikes_with_history\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import LSTMDecoder\n",
    "from Neural_Decoding.decoders import KalmanFilterDecoder\n",
    "from scipy.optimize import curve_fit\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb2b8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = {}\n",
    "mat = hdf5storage.loadmat('indy_20170131_02.mat')\n",
    "for k, v in mat.items():\n",
    "    m[k] = np.transpose(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf6e3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = m['spikes']\n",
    "time = m['t'].squeeze()\n",
    "loca = m['finger_pos']\n",
    "cursor = m['cursor_pos']\n",
    "target = m['target_pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83dd081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1973, 1)\n",
      "(879, 1)\n",
      "(0, 0)\n",
      "(0, 0)\n",
      "(0, 0)\n"
     ]
    }
   ],
   "source": [
    "print(tmp[0][0].shape)\n",
    "print(tmp[1][4].shape)\n",
    "print(tmp[2][4].shape)\n",
    "print(tmp[3][4].shape)\n",
    "print(tmp[4][4].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce46a548",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed6fcf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(tmp.shape[1]):\n",
    "    for i in range(tmp.shape[0]):\n",
    "        if (tmp[i][j].shape[0] != 0):\n",
    "            spikes.append(tmp[i][j].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccc2a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_raster(spikes):\n",
    "    n_neurons = len(spikes)\n",
    "    ax = plt.gca()\n",
    "    colors = 'r'\n",
    "\n",
    "    kwargs = {}\n",
    "    kwargs.setdefault(\"linestyle\", \"None\")\n",
    "    kwargs.setdefault(\"marker\", \"|\")\n",
    "    # Default markersize determined by matching eventplot\n",
    "    fig = ax.figure\n",
    "    bbox = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
    "    ax_height = bbox.height * fig.dpi\n",
    "    markersize = max(ax_height * 0.8 / n_neurons, 1)\n",
    "    # For 1 - 3 neurons, we need an extra fudge factor to match eventplot\n",
    "    markersize -= max(4 - n_neurons, 0) ** 2 * ax_height * 0.005\n",
    "    kwargs.setdefault(\"markersize\", markersize)\n",
    "    kwargs.setdefault(\"markeredgewidth\", 1)\n",
    "\n",
    "    for i in range(n_neurons):\n",
    "        spiketimes = spikes[i].ravel()\n",
    "        ax.plot(\n",
    "            spiketimes,\n",
    "            np.zeros_like(spiketimes) + (i + 1),\n",
    "            color=colors,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    # --- set axes limits\n",
    "    ax.set_xlim(1650, 2500)\n",
    "    ax.set_ylim(0, n_neurons)\n",
    "    ax.set_xlabel('time')\n",
    "    ax.set_title('raster')\n",
    "    ax.set_ylabel('unit')\n",
    "    plt.xlim(1650, 2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d5f1ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_psth(bins, dt, t_start, t_end):\n",
    "    # Plot the PSTH\n",
    "    x = np.arange(t_start, t_end - dt, dt)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x, bins, align='edge')\n",
    "    ax.set_xlabel('Time (sec)')\n",
    "    ax.set_ylabel('Number of spikes/bin')\n",
    "    ax.set_title('Peri-Stimulus Time Histogram')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cb274fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dif(arr, axis, period):\n",
    "    nd = arr.ndim\n",
    "    slice1 = [slice(None)] * nd\n",
    "    slice2 = [slice(None)] * nd\n",
    "    slice1[axis] = slice(period, None)\n",
    "    slice2[axis] = slice(None, 0 - period)\n",
    "    slice1 = tuple(slice1)\n",
    "    slice2 = tuple(slice2)\n",
    "    return np.subtract(arr[slice1], arr[slice2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7aa80a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loca_diff = dif(loca, 1, 2)\n",
    "time_diff = dif(time, 0, 2)\n",
    "curs_diff = dif(cursor, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3f1bac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel = np.divide(loca_diff, time_diff)\n",
    "curs_vel = np.divide(curs_diff, time_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b2568b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vel=np.concatenate((vel[:, 0 : 1], vel, vel[:, -1 :]),axis=1)\n",
    "curs_vel=np.concatenate((curs_vel[:, 0 : 1], curs_vel, curs_vel[:, -1 :]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5cd81ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel = vel.T\n",
    "curs_vel = curs_vel.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b85e1be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=0.1 #Size of time bins (in seconds)\n",
    "t_start=time[0] #Time to start extracting data - here the first time velocity was recorded\n",
    "t_end=time[-1] #Time to finish extracting data - here the last time velocity was recorded\n",
    "downsample_factor=1 #Downsampling of output (to make binning go faster). 1 means no downsampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "e7987dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = np.array(spikes, dtype = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "4340fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(spikes.shape[0]):\n",
    "    spikes[i]=np.squeeze(spikes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "afe522d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Preprocessing to put spikes and output in bins###\n",
    "\n",
    "#Bin neural data using \"bin_spikes\" function\n",
    "neural_data=bin_spikes(spikes,dt,t_start,t_end)\n",
    "\n",
    "#Bin output (velocity/position) data using \"bin_output\" function\n",
    "vels_binned=bin_output(vel,time,dt,t_start,t_end,downsample_factor)\n",
    "curs_vel_binned=bin_output(curs_vel,time,dt,t_start,t_end,downsample_factor)\n",
    "pos_binned=bin_output(loca.T,time,dt,t_start,t_end,downsample_factor)\n",
    "# target_binned=bin_output(target.T,time,dt,t_start,t_end,downsample_factor)\n",
    "# curs_binned=bin_output(cursor.T,time,dt,t_start,t_end,downsample_factor)\n",
    "\n",
    "#We will now determine acceleration    \n",
    "temp=np.diff(vels_binned,axis=0) #The acceleration is the difference in velocities across time bins \n",
    "acc_binned=np.concatenate((temp,temp[-1:,:]),axis=0) #Assume acceleration at last time point is same as 2nd to last\n",
    "#We will now determine acceleration for cursor\n",
    "temp=np.diff(curs_vel_binned,axis=0) #The acceleration is the difference in velocities across time bins \n",
    "curs_acc_binned=np.concatenate((temp,temp[-1:,:]),axis=0) #Assume acceleration at last time point is same as 2nd to last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "3a366fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caculate all spikes\n",
    "sum_nur = np.sum(neural_data, axis=1)\n",
    "plot_psth(sum_nur, dt, t_start, t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "7480885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ve_num = np.sqrt(np.add(np.multiply(curs_vel_binned[:, 0], curs_vel_binned[:, 0]), np.multiply(curs_vel_binned[:, 1], curs_vel_binned[:, 1])))\n",
    "acc_num = np.sqrt(np.add(np.multiply(curs_acc_binned[:, 0], curs_acc_binned[:, 0]), np.multiply(curs_acc_binned[:, 1], curs_acc_binned[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "f045ceda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, a, b, c):\n",
    "  return a * np.exp(-b * x) + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "04a7c16a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#popt, pcov = curve_fit(func, ve_num, neural_data[:, 2])  #for vec\n",
    "#nur_pred = [func(i, popt[0], popt[1], popt[2]) for i in ve_num]\n",
    "popt, pcov = curve_fit(func, acc_num, neural_data[:, 2])  #for acc\n",
    "nur_pred = [func(i, popt[0], popt[1], popt[2]) for i in acc_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "f9b691f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据点与原先的进行画图比较\n",
    "ax = plt.subplot()\n",
    "plt.plot(acc_num, neural_data[:, 2], '*', label='original values')\n",
    "plt.plot(acc_num, nur_pred, 'r', label='fit values')\n",
    "ax.set_xlabel('Acc')\n",
    "ax.set_ylabel('Number of spikes')\n",
    "ax.set_title('Tunning curve')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "11e48f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task2 find the units with low r2_score\n",
    "del_ind = []\n",
    "sc = []\n",
    "std_vels = StandardScaler().fit_transform(vels_binned)\n",
    "std_neu = StandardScaler().fit_transform(neural_data)\n",
    "for i in range(neural_data.shape[1]):\n",
    "    reg = LinearRegression().fit(std_vels, std_neu[:,i])\n",
    "    r2 = reg.score(std_vels, std_neu[:,i])\n",
    "    if ( r2 < 0.1):\n",
    "        del_ind.append(i)\n",
    "        sc.append(r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "64350c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plt.subplot()\n",
    "plt.plot(del_ind, sc, 'r')\n",
    "ax.set_xlabel('unit id')\n",
    "ax.set_ylabel('R2')\n",
    "ax.set_title('R2 scores')\n",
    "plt.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "0099981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sco = np.array(sc)\n",
    "p7 = np.percentile(sco, 35)\n",
    "del_ind = np.array(del_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "e20bcce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_data = np.delete(neural_data, del_ind[sco < p7], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "3f7dd10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 203995)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8159, 174)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(loca.shape)\n",
    "neural_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "81889c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#task 3\n",
    "#The covariate is simply the matrix of firing rates for all neurons over time\n",
    "X=neural_data\n",
    "#The final output covariates include position, velocity, and acceleration\n",
    "#y=np.concatenate((pos_binned,vels_binned,acc_binned),axis=1)  #pos、vec、acc\n",
    "#y=np.concatenate((pos_binned,vels_binned),axis=1)  #pos、vec\n",
    "#y=pos_binned  #pos\n",
    "#y=vels_binned  #pos  #vec\n",
    "y=acc_binned  #pos  #acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4d5b43d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(X, y):\n",
    "    #Set what part of data should be part of the training/testing/validation sets\n",
    "    training_range=[0, 0.7]\n",
    "    testing_range=[0.7, 0.85]\n",
    "    valid_range=[0.85,1]\n",
    "    #Number of examples after taking into account bins removed for lag alignment\n",
    "    num_examples=X.shape[0]\n",
    "\n",
    "    #Note that each range has a buffer of 1 bin at the beginning and end\n",
    "    #This makes it so that the different sets don't include overlapping data\n",
    "    training_set=np.arange(np.int_(np.round(training_range[0]*num_examples))+1,np.int_(np.round(training_range[1]*num_examples))-1)\n",
    "    testing_set=np.arange(np.int_(np.round(testing_range[0]*num_examples))+1,np.int_(np.round(testing_range[1]*num_examples))-1)\n",
    "    valid_set=np.arange(np.int_(np.round(valid_range[0]*num_examples))+1,np.int_(np.round(valid_range[1]*num_examples))-1)\n",
    "\n",
    "    #Get training data\n",
    "    X_train=X[training_set,:]\n",
    "    y_train=y[training_set,:]\n",
    "\n",
    "    #Get testing data\n",
    "    X_test=X[testing_set,:]\n",
    "    y_test=y[testing_set,:]\n",
    "\n",
    "    #Get validation data\n",
    "    X_valid=X[valid_set,:]\n",
    "    y_valid=y[valid_set,:]\n",
    "    #Z-score inputs \n",
    "    X_train_mean=np.nanmean(X_train,axis=0)\n",
    "    X_train_std=np.nanstd(X_train,axis=0)\n",
    "    X_train=(X_train-X_train_mean)/X_train_std\n",
    "    X_test=(X_test-X_train_mean)/X_train_std\n",
    "    X_valid=(X_valid-X_train_mean)/X_train_std\n",
    "\n",
    "    #Zero-center outputs\n",
    "    y_train_mean=np.mean(y_train,axis=0)\n",
    "    y_train=y_train-y_train_mean\n",
    "    y_test=y_test-y_train_mean\n",
    "    y_valid=y_valid-y_train_mean\n",
    "    return [X_train, X_test, X_valid, y_train, y_test, y_valid, y_train_mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "0cca0e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, X_valid, y_train, y_test, y_valid, y_train_mean = process_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "24072a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.15661480483391566\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model=KalmanFilterDecoder(C=3) #There is one optional parameter that is set to the default in this example (see ReadMe)\n",
    "\n",
    "#Fit model\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted=model.predict(X_valid,y_valid)\n",
    "\n",
    "#Get metrics of fit (see read me for more details on the differences between metrics)\n",
    "#First I'll get the R^2\n",
    "R2=r2_score(y_valid,y_valid_predicted)\n",
    "print('R2:',R2) #I'm just printing the R^2's of the 3rd and 4th entries that correspond to the velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59b23962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x288ad78ae50>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As an example, I plot an example 1000 values of the x velocity (column index 2), both true and predicted with the Kalman filter\n",
    "#Note that I add back in the mean value, so that both true and predicted values are in the original coordinates\n",
    "fig_x=plt.figure()\n",
    "plt.plot(y_valid[1000:2000,2]+y_train_mean[2],'b')\n",
    "plt.plot(y_valid_predicted[1000:2000,2]+y_train_mean[2],'r')\n",
    "#Save figure\n",
    "# fig_x.savefig('x_velocity_decoding.eps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "596e4d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins_before=0 #How many bins of neural data prior to the output are used for decoding\n",
    "bins_current=1 #Whether to use concurrent time bin of neural data\n",
    "bins_after=0 #How many bins of neural data after the output are used for decoding\n",
    "# Format for recurrent neural networks (SimpleRNN, GRU, LSTM)\n",
    "# Function to get the covariate matrix that includes spike history from previous bins\n",
    "X=get_spikes_with_history(neural_data,bins_before,bins_after,bins_current)\n",
    "y=vels_binned\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, y_train_mean = process_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "cba14140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39/39 [==============================] - 0s 2ms/step\n",
      "R2: 0.4266074244179127\n"
     ]
    }
   ],
   "source": [
    "#Declare model\n",
    "model_lstm=LSTMDecoder(units=400,dropout=0,num_epochs=5)\n",
    "\n",
    "#Fit model\n",
    "model_lstm.fit(X_train,y_train)\n",
    "\n",
    "#Get predictions\n",
    "y_valid_predicted_lstm=model_lstm.predict(X_valid)\n",
    "\n",
    "#Get metric of fit\n",
    "R2_lstm=r2_score(y_valid,y_valid_predicted_lstm)\n",
    "print('R2:', R2_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "3b1c3f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.3277089127111392\n"
     ]
    }
   ],
   "source": [
    "X = neural_data\n",
    "y = vels_binned\n",
    "X_train, X_test, X_valid, y_train, y_test, y_valid, y_train_mean = process_data(X, y)\n",
    "line_mod = LinearRegression().fit(X_train, y_train)\n",
    "y_valid_predicted_line = line_mod.predict(X_valid)\n",
    "R2_line = r2_score(y_valid, y_valid_predicted_line)\n",
    "print('R2:', R2_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ae328e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
